{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOldPfcJyZAAV2RYlBUFnHj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alokamgnaneswarasai/Linear-regression-using-gradient-descent/blob/main/basicnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "Iog_0OuwXKXs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us construct a basic Network"
      ],
      "metadata": {
        "id": "hvyNqBMSYkqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    self.fc1=nn.Linear(10,100)\n",
        "    self.fc2=nn.Linear(100,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.fc1(x)\n",
        "    x=F.sigmoid(x)\n",
        "    x=self.fc2(x)\n",
        "    x=F.softmax(x)\n",
        "    return x\n",
        "\n",
        "net=Net()\n",
        "print(net)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EGmUrApYDs2",
        "outputId": "dedd9065-eba7-4c20-d471-7bdbc66d42cb"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=10, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us check the parameters of this network"
      ],
      "metadata": {
        "id": "a9lC-XulZ1Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params=list(net.parameters())\n",
        "print(params[0].size()) # W matrix between input and hidden layer\n",
        "print(params[1].size()) # bias at hidden layer\n",
        "print(params[2].size()) # W matrix between hidden and output layer\n",
        "print(params[3].size()) # bias at output layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C98YSloZ49M",
        "outputId": "7cc39539-b85a-4931-d197-f619e279eba5"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 10])\n",
            "torch.Size([100])\n",
            "torch.Size([10, 100])\n",
            "torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now let us call the forward function and compute loss along with that**"
      ],
      "metadata": {
        "id": "_Vapk_P_a9ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input=torch.randn(10)\n",
        "target=torch.randn(10)\n",
        "output=net(input)\n",
        "print(f\"Output is {output}\")\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "loss=criterion(target,output)\n",
        "print(f\"The loss is {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzzZGHENbGAM",
        "outputId": "159a16ef-34eb-4b85-8f32-fa3fb65c6804"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output is tensor([0.0905, 0.0705, 0.1121, 0.1003, 0.0712, 0.0988, 0.0917, 0.1088, 0.1611,\n",
            "        0.0951], grad_fn=<SoftmaxBackward0>)\n",
            "The loss is 2.661743640899658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-98-3f4527fba34f>:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x=F.softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now let us write the backprop code and add optimizer which helps in updating the parameter weights**"
      ],
      "metadata": {
        "id": "rH18A3gscNf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "while(loss>1.5):\n",
        "  optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
        "  optimizer.zero_grad()\n",
        "  output = net(input)\n",
        "  loss = criterion(target,output)\n",
        "\n",
        "  # optimizer.zero_grad()  # Zero the gradients before backward pass\n",
        "  loss.backward()        # Backward pass to compute gradients\n",
        "  optimizer.step()       # Update weights based on gradients\n",
        "  print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBtxqrqqcZP8",
        "outputId": "c7dec1b3-f7f0-41d9-9880-c2f3f1bbca28"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.6617, grad_fn=<DivBackward1>)\n",
            "tensor(2.3164, grad_fn=<DivBackward1>)\n",
            "tensor(2.0237, grad_fn=<DivBackward1>)\n",
            "tensor(1.8572, grad_fn=<DivBackward1>)\n",
            "tensor(1.7469, grad_fn=<DivBackward1>)\n",
            "tensor(1.6597, grad_fn=<DivBackward1>)\n",
            "tensor(1.5798, grad_fn=<DivBackward1>)\n",
            "tensor(1.5271, grad_fn=<DivBackward1>)\n",
            "tensor(1.5054, grad_fn=<DivBackward1>)\n",
            "tensor(1.4985, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-98-3f4527fba34f>:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x=F.softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=criterion(target,net(input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7CnxaT7e5RX",
        "outputId": "5505c8a6-4610-4a22-b9ea-bc62b356fdb7"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-98-3f4527fba34f>:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x=F.softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d0qTHFHlxzv",
        "outputId": "d19b0510-ca14-45ad-9fb0-83deed36c876"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.4965, grad_fn=<DivBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HvKNlYZJlziY"
      },
      "execution_count": 103,
      "outputs": []
    }
  ]
}